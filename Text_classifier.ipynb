{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMApQJjPU4RjlZ/42flqdXq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarthakbiswas97/design-llm-apps-exercises/blob/main/Text_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbTUi6-3w5tb",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install wikipedia\n",
        "!pip install --upgrade datasets fsspec"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Getting and preparing data **"
      ],
      "metadata": {
        "id": "5TbiE41Vhwnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "from datasets import load_dataset\n",
        "from itertools import islice\n",
        "\n",
        "# 1) Configuration\n",
        "C4_SAMPLES    = 1000\n",
        "WIKI_SAMPLES  = 1000\n",
        "RAW_FILE      = \"combined.txt\"\n",
        "SHUFFLED_FILE = \"combined_shuffled.txt\"\n",
        "TRAIN_FILE    = \"train.txt\"\n",
        "VALID_FILE    = \"valid.txt\"\n",
        "BAD_LABEL     = \"__label__bad\"\n",
        "GOOD_LABEL    = \"__label__good\"\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    1) strip whitespace\n",
        "    2) lowercase\n",
        "    3) remove non-aâ€“z characters\n",
        "    4) collapse all whitespace to single spaces\n",
        "    \"\"\"\n",
        "    text = text.strip().lower()\n",
        "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
        "    text = \" \".join(text.split())\n",
        "    return text\n",
        "\n",
        "# 2) Write raw, labeled examples to RAW_FILE\n",
        "with open(RAW_FILE, \"w\", encoding=\"utf-8\") as out:\n",
        "    # 2a) C4 \"bad\" examples\n",
        "    c4_iter = load_dataset(\n",
        "        \"allenai/c4\", \"en.noclean\", streaming=True\n",
        "    )[\"train\"]\n",
        "    for rec in islice(c4_iter, C4_SAMPLES):\n",
        "        txt = clean_text(rec[\"text\"])\n",
        "        if txt:\n",
        "            out.write(f\"{BAD_LABEL} {txt}\\n\")\n",
        "\n",
        "    # 2b) Wikipedia \"good\" examples via HF snapshot\n",
        "    # Use config \"20220301.en\" for English wiki\n",
        "    wiki_stream = load_dataset(\n",
        "        \"wikipedia\",\n",
        "        \"20220301.en\",\n",
        "        streaming=True,\n",
        "        trust_remote_code=True\n",
        "    )[\"train\"]\n",
        "    # shuffle a buffer of 10k, then take WIKI_SAMPLES\n",
        "    wiki_shuf = wiki_stream.shuffle(buffer_size=10_000, seed=42)\n",
        "    for rec in islice(wiki_shuf, WIKI_SAMPLES):\n",
        "        # take only the first paragraph\n",
        "        first_para = rec[\"text\"].split(\"\\n\\n\", 1)[0]\n",
        "        txt = clean_text(first_para)\n",
        "        if txt:\n",
        "            out.write(f\"{GOOD_LABEL} {txt}\\n\")\n",
        "\n",
        "# 3) Read all lines back and shuffle in memory\n",
        "with open(RAW_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.readlines()\n",
        "random.shuffle(lines)\n",
        "\n",
        "# 4) Write out the shuffled dataset\n",
        "with open(SHUFFLED_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.writelines(lines)\n",
        "\n",
        "# 5) Split 80/20 into train.txt and valid.txt\n",
        "split_idx = int(len(lines) * 0.8)\n",
        "with open(TRAIN_FILE, \"w\", encoding=\"utf-8\") as f_train:\n",
        "    f_train.writelines(lines[:split_idx])\n",
        "with open(VALID_FILE, \"w\", encoding=\"utf-8\") as f_valid:\n",
        "    f_valid.writelines(lines[split_idx:])\n",
        "\n",
        "# 6) Sanity check: print a few samples\n",
        "print(\"=== TRAIN SAMPLE ===\")\n",
        "for ln in lines[:5]:\n",
        "    print(ln.strip())\n",
        "print(\"\\n=== VALID SAMPLE ===\")\n",
        "for ln in lines[split_idx : split_idx + 5]:\n",
        "    print(ln.strip())\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gMJ4pgzi0Gi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model training**"
      ],
      "metadata": {
        "id": "x1LfSyClkCQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade fasttext\n",
        "!pip install numpy==1.26.4"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BP0-svD0kJNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "import numpy as np\n",
        "\n",
        "# print(f\"Using NumPy version: {np.__version__}\")\n",
        "\n",
        "MODEL_OUTPUT_FILE = \"quality_classifier.bin\"\n",
        "\n",
        "print(\"Starting model training...\")\n",
        "model = fasttext.train_supervised(\n",
        "    input=TRAIN_FILE,\n",
        "    lr=0.5,\n",
        "    epoch=25,\n",
        "    wordNgrams=2,\n",
        "    dim=150,\n",
        "    # autotuneValidationFile=VALID_FILE\n",
        "    # autotuneDuration=600,             # e.g., 10 minutes for autotune\n",
        "    # autotuneMetric=\"f1\",\n",
        "    verbose=2 # To see training progress\n",
        ")\n",
        "print(\"Training complete.\")\n",
        "\n",
        "# Save the trained model\n",
        "model.save_model(MODEL_OUTPUT_FILE)\n",
        "print(f\"Model saved to {MODEL_OUTPUT_FILE}\")\n",
        "\n",
        "# Evaluate on your validation set\n",
        "print(\"\\nEvaluating on the validation set:\")\n",
        "N, P, R = model.test(VALID_FILE)\n",
        "print(f\"Validation N: {N}\")\n",
        "print(f\"Validation P@1: {P:.4f}\")\n",
        "print(f\"Validation R@1: {R:.4f}\")\n",
        "\n",
        "# If you used autotune, you can see the best F1 score and hyperparameters\n",
        "# if 'autotuneValidationFile' in model.f.getArgs().__dict__: # Check if autotune was used\n",
        "#     print(f\"Best F1 score achieved by autotune: {model.get_best_f1_score()}\")\n",
        "#     print(f\"Best hyperparameters: {model.get_best_hyperparameters()}\")\n"
      ],
      "metadata": {
        "id": "MfGZ0Z7BzUKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation on \"RealNewsLike\" C4**"
      ],
      "metadata": {
        "id": "qtSUs_Xx3ifI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "REALNEWS_SAMPLES = 10000\n",
        "\n",
        "try:\n",
        "  realnewslike = load_dataset(\"allenai/c4\", \"realnewslike\", streaming=True)[\"train\"]\n",
        "except Exception as e:\n",
        "  print(f\"Error loading realnewslike: {e}\")\n",
        "\n",
        "predictions_on_realnews = []\n",
        "processed_count = 0\n",
        "\n",
        "for rec in islice(realnewslike, REALNEWS_SAMPLES * 2):\n",
        "  if processed_count >= REALNEWS_SAMPLES:\n",
        "    break\n",
        "\n",
        "  original_text = rec.get(\"text\", \"\")\n",
        "  cleaned_text = clean_text(original_text)\n",
        "\n",
        "  if cleaned_text:\n",
        "    predicted_labels, probabilities = model.predict(cleaned_text)\n",
        "    predictions_on_realnews.append({\n",
        "            \"original_text\": original_text[:500] + \"...\", # Store a snippet\n",
        "            \"cleaned_text\": cleaned_text[:500] + \"...\",\n",
        "            \"predicted_label\": predicted_labels[0],\n",
        "            \"probability\": probabilities[0]\n",
        "    })\n",
        "  processed_count += 1\n",
        "\n",
        "print(f\"\\nMade predictions on {len(predictions_on_realnews)} 'realnewslike' samples.\")\n",
        "\n",
        "# Now, proceed to Phase 4: Analysis\n",
        "# For example, print some predictions:\n",
        "print(\"\\n--- Sample Predictions on 'RealNewsLike' C4 ---\")\n",
        "for i, pred in enumerate(predictions_on_realnews[:10]): # Print first 10\n",
        "    print(f\"\\nSample {i+1}:\")\n",
        "    # print(f\"Original Snippet: {pred['original_text']}\")\n",
        "    print(f\"Predicted Label: {pred['predicted_label']} (Confidence: {pred['probability']:.4f})\")\n",
        "    if i < 3: # Print more details for the very first few\n",
        "        print(f\"Cleaned Snippet for Prediction: {pred['cleaned_text'][:200]}...\")\n",
        "\n",
        "\n",
        "# Count the distribution of predictions\n",
        "good_count = sum(1 for p in predictions_on_realnews if p['predicted_label'] == GOOD_LABEL)\n",
        "bad_count = sum(1 for p in predictions_on_realnews if p['predicted_label'] == BAD_LABEL)\n",
        "\n",
        "if predictions_on_realnews:\n",
        "    print(f\"\\nDistribution on 'RealNewsLike' C4 ({len(predictions_on_realnews)} samples):\")\n",
        "    print(f\"Predicted as GOOD: {good_count} ({good_count/len(predictions_on_realnews)*100:.2f}%)\")\n",
        "    print(f\"Predicted as BAD:  {bad_count} ({bad_count/len(predictions_on_realnews)*100:.2f}%)\")\n",
        "else:\n",
        "    print(\"No predictions were made on realnews data.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "k7SeVirm3op6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.  **Created a quality classifier:**\n",
        "    *   Positive examples from Wikipedia.\n",
        "    *   Negative examples from the unclean version of C4.\n",
        "2.  **Trained the classifier** using fastText.\n",
        "3.  **Fed documents from the `realnewslike` subset of C4 to this classifier.**\n",
        "4.  **Gathered results** from this classification.\n",
        "\n",
        "The output you've provided:\n",
        "\n",
        "*   **Made predictions on 10000 'realnewslike' samples.**\n",
        "*   **Distribution on 'RealNewsLike' C4 (10000 samples):**\n",
        "    *   Predicted as GOOD: 877 (8.77%)\n",
        "    *   Predicted as BAD: 9123 (91.23%)\n",
        "\n",
        "This is the crucial piece of information the exercise asked you to find.\n",
        "\n",
        "Now, let's briefly touch upon the final question of the exercise: **\"Is this classifier able to do a good job?\"**\n",
        "\n",
        "Based on your results:\n",
        "\n",
        "*   Your classifier, trained to see Wikipedia as the gold standard for \"good\" and very messy C4 (`en.noclean`) as \"bad\", considers the vast majority (91.23%) of the `realnewslike` C4 subset to be \"bad.\"\n",
        "*   This suggests that, according to the features your model learned (word n-grams, etc.), the `realnewslike` C4 data is significantly different from Wikipedia and more similar to the noisy C4 data it was trained to identify as \"bad.\"\n",
        "\n",
        "Whether this constitutes a \"good job\" is interpretive and depends on the goal:\n",
        "\n",
        "*   **If the goal was to create a very strict filter that only accepts text of extremely high, Wikipedia-like quality:** Then one might argue it's doing a \"good job\" by being highly selective. It's effectively saying that `realnewslike` C4, while cleaner than `en.noclean`, still doesn't meet the bar set by Wikipedia.\n",
        "*   **If the goal was to identify most reasonably well-written news-like articles as \"good\":** Then it might not be doing a \"good job,\" as it's rejecting a large portion of the `realnewslike` dataset. This could mean your definition of \"bad\" (based on `en.noclean`) is too broad or that `realnewslike` C4 has characteristics that your model, trained on the extremes, flags as low quality.\n",
        "\n",
        "**In conclusion:**\n",
        "\n",
        "*   **Exercise Flow:** **Yes, you have completed all the steps outlined in the exercise.**\n",
        "*   **Classifier Performance (\"Good Job?\"):** The classifier is performing *consistently* with its training. It has learned to differentiate Wikipedia from very noisy text, and it's applying that learning to the `realnewslike` subset. The high \"bad\" rate for `realnewslike` is an interesting finding and provides insight into how different these datasets are, at least from the perspective of your model."
      ],
      "metadata": {
        "id": "XsPwvoTd2-o4"
      }
    }
  ]
}